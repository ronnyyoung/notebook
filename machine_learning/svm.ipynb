{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " # 两分类问题\n",
    " \n",
    "数据点用$\\mathbf x$表示，这是一个$n$维向量，而类别用$y$表示，可以取1或-1，分别代表不同的类（有些地方，如逻辑回归里会选0和1，这本质上没有什么不同，只要是可以区分的两个数字）。\n",
    " \n",
    "线性分类器就是要在$n$维的数据空间中找到一个超平面，其方程可以表示为：\n",
    "$$b + w_1x_1 + \\dots + w_nx_n = \\mathbf {w^Tx} + b = 0$$\n",
    " \n",
    "在二维空间中，超平面就变成了一条直线。我们希望的是，通过这个超平面可以把两类数据分隔开，比如，在超平面一边的数据点所对应的$y$全是-1，而另一边全是1。具体来说，我们令$f(\\mathbf x) =\\mathbf {w^Tx} + b $，显然，如果$f(\\mathbf x) = 0$，那么$\\mathbf x$是位于超平面上的点。我们不妨要求对于所有满足$f(\\mathbf x) < 0$的点，其对应的$y$等于$-1$，而$f(\\mathbf x)>0$则对应$y=1$的数据点。\n",
    " \n",
    "这里我的先讨论线性可分的情况！\n",
    " \n",
    "根据上面的规则，我们便得到了一个分类器，对于一个新的数据点$\\mathbf x$，则将其代入得到$f(\\mathbf x)$，如果$f(\\mathbf x) > 0$，则其属于$1$那一类，如果$f(\\mathbf x) < 0$，则其属于$-1$的那一类。\n",
    " \n",
    "但是如果$f(\\mathbf x) = 0$，怎么办呢？分到哪一类都不是。事实上，对于$f(\\mathbf x)$的绝对值很小的情况，我们都很难处理，因为细微的变动就可以导致结果类别的改变。理想的情况下，我们希望$f(\\mathbf x)$的值都是很大正数或很小的负数，这样我们就能更加确信它是属于其中某一类别。\n",
    " \n",
    "我们定义**Functional Margin**为\n",
    "$$\\hat{\\gamma}=y(\\mathbf{w^Tx}+b)=yf(\\mathbf x)$$\n",
    " \n",
    "注意前面乘上类别 $y$之后可以保证这个margin的非负性，而点到超平面的距离定义为**geometrical margin**。\n",
    "\n",
    " \n",
    "不妨来看看二者之间的关系。如图所示，对于一个点$\\mathbf x$，令其垂直投影到超平面上的对应的为$\\mathbf x_0$，由于$\\mathbf w$是垂直于超平面的一个向量，我们有\n",
    " \n",
    "$$\\mathbf x=\\mathbf x_0+\\gamma\\frac{\\mathbf w}{\\|\\mathbf w\\|}$$\n",
    " \n",
    "又由于$\\mathbf x_0$是超平面上的点，满足 $\\mathbf {w^Tx} + b=0$，则在上面的等式中等号左右同时乘以$\\mathbf w^T$，则有：\n",
    "$$\\mathbf{w^Tx} = \\mathbf{w^Tx_0} + \\gamma\\frac{\\mathbf{w^Tw}}{\\|\\mathbf w\\|}$$\n",
    "进一步推导到到：\n",
    "$$ \\gamma = \\frac{\\mathbf{w^Tx}+b}{\\|\\mathbf w\\|}=\\frac{f(\\mathbf  x)}{\\| \\mathbf  w\\|} $$\n",
    " \n",
    "不过，这里的 $\\gamma$是带符号的，我们需要的只是它的绝对值，因此类似地，也乘上对应的类别$y$即可，因此实际上我们定义**geometrical margin**为：\n",
    "$$\\tilde{\\gamma} = y\\gamma = \\frac{\\hat{\\gamma}}{\\| \\mathbf  w\\|}$$\n",
    " \n",
    "显然，函数间隔和几何间隔相差一个$\\|w\\|$的缩放因子。按照我们前面的分析，对一个数据点进行分类，当它的margin越大的时候，分类的confidence越大。对于一个包含$n$个点的数据集，我们可以很自然地定义它的margin为所有这$n$个点的margin值中最小的那个。于是，为了使得分类的confidence高，我们希望所选择的hyper plane能够最大化这个margin值。\n",
    "\n",
    "不过这里我们有两个margin可以选，不过functional margin明显是不太适合用来最大化的一个量，因为在hyper plane固定以后，我们可以等比例地缩放$\\mathbf  w$的长度和$b$ 的值，这样可以使得 $f(x)= \\mathbf{w^Tx}+b$ 的值任意大，亦即 functional margin $\\hat{\\gamma}$ 可以在 hyper plane 保持不变的情况下被取得任意大，而 geometrical margin 则没有这个问题，因为除上了 $\\|w\\|$ 这个分母，所以缩放 $\\mathbf  w$ 和 $b$ 的时候 $\\tilde{\\gamma}$ 的值是不会改变的，它只随着 hyper plane 的变动而变动，因此，这是更加合适的一个 margin 。\n",
    " \n",
    "这样一来，我们的 maximum margin classifier 的目标函数即定义为$\\max \\tilde{\\gamma}$,当然，还需要满足一些条件，根据margin的定义，我们有$ y_i(w^Tx_i+b) = \\hat{\\gamma}_i \\geq \\hat{\\gamma}, \\quad i=1,\\ldots,n $其中 $\\hat{\\gamma}=\\tilde{\\gamma}\\|w\\|$ ，根据我们刚才的讨论，即使在超平面固定的情况下，$\\hat{\\gamma}$ 的值也可以随着 $\\|w\\|$ 的变化而变化。由于我们的目标就是要确定超平面，因此可以把这个无关的变量固定下来，固定的方式有两种：一是固定 $\\|w\\|$ ，当我们找到最优的 $\\tilde{\\gamma}$ 时 $\\hat{\\gamma}$ 也就可以随之而固定；二是反过来固定 $\\hat{\\gamma}$ ，此时 $\\|w\\|$ 也可以根据最优的 $\\tilde{\\gamma}$ 得到。处于方便推导和优化的目的，我们选择第二种，令 $\\hat{\\gamma}=1$ ，则我们的目标函数化为：\n",
    "$$\\max \\frac{1}{\\|w\\|}, \\quad s.t., y_i(w^Tx_i+b)\\geq 1, i=1,\\ldots,n $$\n",
    "\n",
    "通过求解这个问题，我们就可以找到一个 margin 最大的classifier ，如下图所示，中间的红色线条是 Optimal Hyper Plane ，另外两条线到红线的距离都是等于 $\\tilde{\\gamma}$ 的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
