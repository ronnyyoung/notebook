{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "很多机器学习算法，最后都演化为了优化问题。在有监督学习中，我们总是定义预测值与真实值之间的损失函数，我们的优化目标就是找到一组参数来使得损失函数达到最小。像之前介绍过的Least-Squares, Logistic Regression, 以及Support Vector Machines都可以看成为对损失函数的优化问题。\n",
    "\n",
    "事实证明，想求一个函数的全局最值是一件非常困难的事情。然后，对于有一类优化问题，我们称它为**convex optimization problems**，我们可以很efficiently 找到全局最优解。这里的efficiently是指对于这一类的优化问题，我们可以在一个多项式时间（polynomially）内求解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convex Sets\n",
    "\n",
    "**Definition** $C$ is convex if, for any $x ,y \\in C$ and $\\theta \\in \\mathbf{R}$ with $0 \\le \\theta \\le 1$,\n",
    "$$\\theta x + (1-\\theta)y\\in C$$\n",
    "\n",
    "直觉上讲，就是在一个集合C里的两个点，我们在这两个点之间画一条线段，这条线段上的所有点都属于这个集合。线段上的点$\\theta x + (1-\\theta)y$也被叫作点$x$与点$y$的**convex combination**。\n",
    "\n",
    "![](images/notebook_ml_co_001.png)\n",
    "\n",
    "Convex Sets的一些例子：\n",
    "\n",
    "- All of $\\mathbb{R^n}$\n",
    "- set $\\{x\\in\\mathbb{R^n}:Ax = b\\}$\n",
    "- set $\\{x\\in\\mathbb{R^n}:Ax \\preceq b\\}$\n",
    "- 凸集的交集还是凸集，但凸集的并集并不一定是凸集\n",
    "- 半正定矩阵、正定矩阵，负定矩阵，半负定矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Convex Function\n",
    "\n",
    "**Definition** A function $f:\\mathbb{R^n}\\to\\mathbb{R}$ is convex if its domain (denoted $\\mathcal{D}(f))$ is a convex set , and if, for all $x,y\\in\\mathcal{D}$ and $\\theta\\in\\mathbb{R}, 0\\le\\theta\\le1$,\n",
    "$$f(\\theta x+(1-\\theta)y) \\le \\theta f(x) + (1-\\theta)f(y)$$\n",
    "\n",
    "![Graph of a convex function](images/notebook_ml_co_002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Fisrt order Condition for Convexity\n",
    "\n",
    "Suppose a function $f:\\mathbb{R}^n\\to\\mathbb{R}$ is differentiable (i.e, the gradient $\\nabla_x f(x)$ exists at all points $x$ in the domain of $f$). Then $f$ is convex and only if $\\mathcal{D}(f)$ is a convex set and for all $x,y\\in\\mathcal{D}(f)$,\n",
    "$$f(y) \\ge f(x) + \\nabla_xf(x)^T(y-x)$$\n",
    "Then the function $f(x)+\\nabla_xf(x)^T(y-x)$ is called **first-order approximation** to the function f at the point $x$.\n",
    "\n",
    "凸函数的一阶条件实际上就是说函数的切线在整个定义域上都小于函数值，换句话说，我们以函数上任意一点画一条切线，那么这条切线上的所有点都在该点函数值以下。\n",
    "\n",
    "如果上面的不等式是一个严格的小于关系，则$f$是一个**strictly convex**。\n",
    "\n",
    "![](images/notebook_ml_co_003.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Order Condition for Convexity\n",
    "\n",
    "Suppose a function $f:\\mathbb{R}^n\\to\\mathbb{R}$ is twice differentiable (i.e, the Hessian $\\nabla^2_xf(x)$ is defined for all points x in the domian of f). Then $f$ is convex if and only if $\\mathcal{D}(f)$ is a convex set and its Hessian is positive semidefinte. i.e., for any $x\\in\\mathcal{D}(f)$,\n",
    "$$\\nabla^2_xf(x)\\succeq 0.$$\n",
    "\n",
    "在这里'$\\succeq$'符号，当于向量一块使用的时候，一般表示向量里的每个元素都存在着不等关系，而如果是和一个矩阵一起使用时，一般是表示半正定矩阵。\n",
    "\n",
    "在一维的情况下，上面的不等式就变以了函数的二阶层数$f''(x)$总是非负的，也就是一阶导数单调增加。\n",
    "\n",
    "类比于一阶条件，如果函数的Hessian矩阵为正定的，则函数为严格凸的，如果Hessian矩阵为半负定或负定的，则函数为凹函数或严格凹函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jensen's Inequality\n",
    "\n",
    "$$f\\left(\\sum_{i=1}^k\\theta_ix_i\\right)\\le\\sum_{i=1}^k\\theta_if(x_i),\\ for\\ \\sum_{i=1}^k = 1, \\theta_i\\ge 0\\ \\forall i.$$\n",
    "\n",
    "把求和换成积分形式，上面的不等式可以写为：\n",
    "$$f\\left(\\int p(x)xdx\\right) \\le \\int p(x)f(x)dx \\ \\text{for} \\ \\int p(x)dx=1,p(x)\\ge 0\\ \\forall x.$$\n",
    "\n",
    "在上式中因为$p(x)$的积分等于1,所以可以看为是一个概率密度函数，那么上面的式子可以写为概率的方法，即：\n",
    "$$f(\\mathbf{E}\\left[x\\right])\\le\\mathbf{E}\\left[f(x)\\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sublevel Sets\n",
    "\n",
    "凸函数有一个非常重要的子集合叫**$\\alpha$-sublevel set**。对于给定的凸函数$f:\\mathbb{R}^n\\to\\mathbb{R}$和一个实数$\\alpha\\in\\mathbb{R}$，$\\alpha$-sublevel set 定义如下：\n",
    "\n",
    "$$\\{x\\in\\mathcal{D}(f):f(x)\\le \\alpha\\}.$$\n",
    "\n",
    "我们可以很容易的验证$\\alpha$-sublevel set也是一个凸集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要证明一个函数是一个凸函数，可以利用上面的性质，直接用定义来证明或者用一阶或二阶条件，不同的函数可以适用的方法不一样。\n",
    "\n",
    "几个凸函数的例子：\n",
    "\n",
    "- Affine functions. Let $f:\\mathbb{R}^n\\to\\mathbb{R}, f(x) = b^Tx+c$ for some $b\\in \\mathbb{R}^n, c\\in\\mathbb{R}$.\n",
    "- Nonnegative weighted sums of convex functions. Let $f_1,f_2,\\cdots,f_k$ be convex functions and $w_1,w_2,\\cdots, w_k$ be nonnegative real number. Then\n",
    "$$f(x) = \\sum_{i=1}^kw_if_i(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convex Optimization Problems\n",
    "\n",
    "Formally, a convex optimaization problem is an optimization problem of the form\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize} & \\ \\ f(x) \\\\\n",
    "\\text{subject to} & \\ \\ x\\in C\n",
    "\\end{align}\n",
    "$$\n",
    "where $f$ is a convex function, $C$ is a convex set, and $x$ is the optimaization variable. However, since this can be a litte bit vague, we often write it as\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize} & \\ \\ f(x) \\\\\n",
    "\\text{subject to} & \\ \\ g_i(x)\\le0,\\ \\ i = 1,\\cdots,m \\\\\n",
    "&\\ \\ h_i(x) = 0, \\ \\ i = 1,\\cdots,p\n",
    "\\end{align}\n",
    "$$\n",
    "where $f$ is a convex function, $g_i$ are convex functions, and $h_i$ are affine functions, and $x$ is the optimaization variable.\n",
    "\n",
    "我们把一个优化问题的最值表示为$p^*$（有时也称为$f^*$）,它是目标函数在可行域上的一个极小值。\n",
    "$$p^* = \\min\\{f(x):g_i(x)\\le0,i=1,\\cdots,m,h_i(x)=0,i=1,\\cdots p\\}.$$\n",
    "$p^*$有时可能取到$+\\infty$和$+\\infty$,如果可行域为空或者为unbounded below（存在$x_0$，使得$f(x)\\to-\\infty$）\n",
    "\n",
    "我们把$x^*$称为一个最值点，如果$f(x^*) = p^*$。但是需要注意的是，有可能存在多个最值点，即使最值是一个有限值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Optimality in Convex Problems\n",
    "\n",
    "我们有下面两个定义来正式的区分局部极值点和全局极值点\n",
    "\n",
    "**Definition:** A point $x$ is **locally optimal** if it is feasible and if there exists some $R>0$ such that all feasible points $z$ with $\\|x-z\\|_2\\le R$, satisfy $f(x)\\le f(z)$\n",
    "\n",
    "**Definition:** A point $x$ is **globally optimal** if it is feasible and for all feasible point $z$, $f(x)\\le f(z)$.\n",
    "\n",
    "对凸函数的一个非常重要的性质是：**凸函数的局部极值点就是它的全局极值点**[TODO:Proof]\n",
    "\n",
    "证明：反证法\n",
    "\n",
    "根据定义，$x$是一个局部极值点，$y$是一个全局极值点。存在$R>0$，使得$\\|x-z\\|\\le R且f(x)\\le f(z)$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Special Cases of Convex Problems\n",
    "\n",
    "下面我们来讨论几种特定形式的优化问题，这些类型的问题在机器学习中经常会遇到，而且针对这些类型的优化问题，我们已经设计出了一些很有效的求解方法。\n",
    "\n",
    "## Linear Programming. \n",
    "\n",
    "We say that a convex optimization problem is a **linear program**(LP，线性规划), if both the objective function $f$ and inequality constraints $g_i$ are affine function. In other words, these problems have the form\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize} & \\ \\ c^Tx+d \\\\\n",
    "\\text{subject to} & \\ \\ Gx\\preceq h \\\\\n",
    "&\\ \\ Ax = b\n",
    "\\end{align}\n",
    "$$\n",
    "where $x\\in\\mathbb{R}^n$ is the optimization variable ,$c \\in \\mathbb{R}^n, d\\in\\mathbb{R}, G\\in\\mathbb{R}^{m\\times n}, h \\in \\mathbb{R}^m, A\\in \\mathbb{R}^{p\\times n}, b\\in\\mathbb{R}^p$ are defined by problem, and '$\\preceq$' denotes elementwise inequality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Quadratic Programming\n",
    "\n",
    "We say that a convex optimaization problem is a **quadratic program(QP，二次规划问题)** if the inequality constraints $g_i$ are still all affine, bute if the objective function $f$ is a convex quadratic function. In other words, these problems have the form,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize} & \\ \\ \\frac{1}{2}x^TPx+ c^Tx+d \\\\\n",
    "\\text{subject to} & \\ \\ Gx\\preceq h \\\\\n",
    "&\\ \\ Ax = b\n",
    "\\end{align}\n",
    "$$\n",
    "where again $x\\in\\mathbb{R}^n$ is the optimization variable ,$c \\in \\mathbb{R}^n, d\\in\\mathbb{R}, G\\in\\mathbb{R}^{m\\times n}, h \\in \\mathbb{R}^m, A\\in \\mathbb{R}^{p\\times n}, b\\in\\mathbb{R}^p$ are defined by problem, but we also have $P\\in\\mathbb{S}_+^n$, a symmetric positive semidefinite matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratically Constrained Quadratic Programming'\n",
    "\n",
    "We say that a convex optimization problem is a **quadratically constrained quadratic programming(QCQP)** if both the objective function $f$ and the inequality constrains $g_i$ are convex quadratic functions,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize} & \\ \\ \\frac{1}{2}x^TPx+ c^Tx+d \\\\\n",
    "\\text{subject to} & \\ \\ \\frac{1}{2}x^TQ_ix+r_i^Tx+s_i\\le0,\\ i=1,\\cdots,m \\\\\n",
    "&\\ \\ Ax = b\n",
    "\\end{align}\n",
    "$$\n",
    "where as before, $x\\in\\mathbb{R}^n$ is the optimization variable ,$c \\in \\mathbb{R}^n, d\\in\\mathbb{R},  A\\in \\mathbb{R}^{p\\times n}, b\\in\\mathbb{R}^p, P\\in\\mathbb{S}_+^n$, but we also have $Q_i\\in\\mathbb{S}_+^n,r_i\\in\\mathbb{R}^n, s_i\\in\\mathbb{R}, i=1,\\cdots, m$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semidefinite Programming\n",
    "\n",
    "This last example is more complex than the previous ones, so don't worry if it doesn't make much sense a first. Howerver, semidefinite programming is becomming more prevalent in many areas of machine learning research, so you might encounter these at some point, and it is good to have an idea of what they are. We say that a convex optimaization problem is a **semidefinit programming(SDP)** if it is of the form\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize} & \\ \\ tr(CX) \\\\\n",
    "\\text{subject to} & \\ \\ tr(A_iX)=b_i, i= 1,\\cdots,p \\\\\n",
    "&\\ \\ X\\succeq0\n",
    "\\end{align}\n",
    "$$\n",
    "where the symmetric matrix $X\\in \\mathbb{S}^n$ is the optimization variable, the symmetric matrics $C,A_1,\\cdots,A_p\\in \\mathbb{S}^n$ are defined by the problem, and the constraint $X\\succeq 0$means that we are constraining $X$ to be positive semidefinite.\n",
    "\n",
    "Noet: the optimization variable in this problem is a matrix instead of a vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们上面讨论了4种形式的凸优化问题，实际上可以归类为一种，LP问题可以看成是$P=0$时的QP问题。QP问题可以看成是$Q=0$时的QCQP问题。而最终所有的问题实际上都是SP问题的一种特殊情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "需要学会把我们遇到的一些机器学习中的优化问题，转换为一个我们已经知道形式的凸优化问题，找到$P,c,Q,r,s$等，然后利用一些凸优化的工具包来快速的求解。\n",
    "\n",
    "## Support Vector Machines(SVM)\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize} & \\ \\ \\frac{1}{2}\\|w\\|_2^2+C\\sum_{i=1}^m\\xi_i \\\\\n",
    "\\text{subject to} & \\ \\ y^{(i)}(w^Tx^{(i)}+b)\\ge 1-\\xi_i, i= 1,\\cdots,m \\\\\n",
    "&\\ \\ \\xi_i\\ge 0, i = 1,\\cdots,m\n",
    "\\end{align}\n",
    "$$\n",
    "## Constrained least squares.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize} & \\ \\ \\frac{1}{2}\\|Ax-b\\|_2^2 \\\\\n",
    "\\text{subject to} & \\ \\ l\\preceq x\\preceq u\n",
    "\\end{align}\n",
    "$$\n",
    "## Maximum Likelihood for Logistic Regression\n",
    "$$\n",
    "l(\\theta) = \\sum_{i=1}^n\\{y^{(i)}\\ln g(\\theta^Tx^{(i)})+(1-y^{(i)})\\ln(1-g(\\theta^Tx^{(i)}))\\} \\\\\n",
    "\\text{minimize} \\ \\ -l(\\theta)\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
